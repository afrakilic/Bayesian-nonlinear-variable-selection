---
title: "Appendix-A"
author: "Afra Kilic, Supervisor: dr. ir. Joris Mulder"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r result csv files, echo=FALSE}
setwd("~/Desktop/THESIS_DOCS/CONSISTENCY/consistency-results")
linear_samp=read.csv("linear_samp.csv")[,-1]
sigma2_vec_BF=read.csv("sigma2_vec_BF.csv")[,-1]
n_vec_BF=read.csv("n_vec_BF.csv")[,-1] ; names(n_vec_BF) = c(rep("M11", 4))
linear_beta=read.csv("linear_beta.csv")[,-1]
linear_sigma=read.csv("linear_sigma.csv")[,-1]
B0_samp=read.csv("B01_samp.csv")[,-1]
nonlinear_beta=read.csv("nonlinear_beta.csv")[,-1]
B0_sigma=read.csv("B0_sigma.csv")[,-1]
null_samp=read.csv("null_samp.csv")[,-1]
null_sigma=read.csv("null_sigma.csv")[,-1]
beta_vec_BF = as.vector(read.csv("beta_vec_BF.csv")[,2]); names(beta_vec_BF) = c(rep("M11", 4), "M8", "M8", rep("M11", 4))
```
## Bayes Factor Consistency Check

In this work, a computationally efficient Bayesian variable selection method is presented for linear and nonlinear effects in regression models where a predictor (or feature) can have a zero effect, a non-zero linear effect or non-zero non-linear effect on the outcome variables.  

The proposed method can be used to determine the type of the relationship between the variables and identification of the effect type is important for two reasons. First, variable selection is an important step as redundant predictors will add noise when estimating or testing other quantities of interest. Second, linearity is a key assumption in most of the statistical models. In practice, researchers tend to eyeball to the relationship between the variables based on a scatter plot and in case of observing a non-linearity; linear transformations (e.g., polynomial, logarithmic) are applied. This approach leads several problems in statistical analyses: 

* Linear transformation is only applicable to limited number of nonlinear relationships.
* Statistical tests are applied to transformed variables such that the linear relationship between the transformed variable ($X^2$) and the outcome variable ($Y$) is analyzed.
* performing several different significance tests  on different transformed variables may lead to inflated Type-I error and $p$-hacking.
* Eyeballing is subjective so a principled and probabilistic method to identify the nonlinearity is needed. 

Bayesian approach is used as it is a better way to quantify the relative evidence in data but it comes with two main challenges : prior specification & computation. LaPlace approximation of marginal likelihood via Bayesian Information Criteria (BIC) is used for the proposed method as BIC approximation has computational advantage and it does not require to specify prior distribution because it implicitly assumes unit information prior (Wagenmakers, 2007).

As an example, consider a model with two variables;

$y = f_{1}(x_{1}) + \beta_{2}x_{2} + \epsilon$, \quad where $\epsilon \sim N(0, \sigma^2)$. $f_{1}$ defines the nonlinear relationship between $x_{1}$ and $y$;  $\beta_{2} = a \in \mathbb{R}$ stands for the (non)zero (linear) relationship between $x_{2}$ and $y$. 

There are two predictor variables ($x_{1}$ and $x_{2}$) that can have three possible different effect on the outcome variables. Thus, $3^{2}$ possible models to be specified;

\

\begin{equation}
\tag{1}
     M_{1}: Y \sim 1; \quad M_{2}: Y \sim x_{1}; \quad M_{3}: Y \sim s(x_{1})\\
     M_{4}: Y \sim x_{2}; \quad
     M_{5}: Y \sim s(x_{2}); \quad
     M_{6}: Y \sim x_{1} + x_{2} \\
     M_{7}: Y \sim s(x_{1}) + x_{2}; \quad
     M_{8}: Y \sim x_{1} + s(x_{2}); \quad
     M_{9}: Y \sim s(x_{1}) + s(x_{2})
\end{equation}

\

`s()` represents the smooth term in the `gam` function from the R package `mgcv` and the nonlinear relationship between the predictor and the outcome variables. `s()` is actually a function used in definition of smooth terms within `gam` model formula such as cubic regression splines, Duchon splines or B-splines. It is important to note that without specifying any smooth term, `gam` will be identical to `lm` function such that `lm(y~x)` will fit exactly the same model with `gam(y~x)`. We use the default smooth term of `s()`, thin-plate regression spline (Wood, 2003), to define nonlinear relationship between the predictor and outcome variables. Thin plate regression spline estimates a smoothing function $f$ that minimizes a penalized least squares function $g$:

\begin{equation}
\tag{2}
g(x, y, \lambda) = \sum_{i = 1} ^N (y_{i}- f(x_{i})^{2}) + \lambda J(f)
\end{equation}

where $x$ is the predictor variable, $y$ is the outcome variable, $N$ is the sample size and $J$ is a function that penalizes how wiggly/complex the function $f$ is. $\lambda$ is the penalization parameter and estimated via generalized cross validation (GCV); As λ→∞, the result is a linear fit and as λ→0 we have the opposite effect where any wiggliness is incorporated into the model. To use this function we also need to specify `k` which controls the number of basis function for $f$ and thus is another way to control the degree of wiggliness of the function. Exact choice of `k` is not generally critical such that it should be chosen to be sufficiently large to capture the underlying nonlinear relationship, yet small enough to maintain acceptable computational efficiency. We specify the smooth function with `k = 4` to ease the computation and it is sufficient to capture the nonlinear relationships defined in this study.  

#### Bayes Factor Calculation

After specifying all possible models, the relative evidence in the data in favor of each model can be calculated by using the $BF_{ij}$ and the best model will be selected by means of posterior model probabilities. $BF_{ij}$ is calculated by using LaPlace approximation of marginal likelihood via BIC as in the following: 

\begin{equation}
\tag{3}
BF_{ij} \approx \frac{Pr_{BIC}(y|M_{i})}{Pr_{BIC}(y|M_{j})} = exp(\frac{BIC({H}_{j}) - BIC({H}_{i})}{2})
\end{equation}

BIC for model $M_{i}$ is defined as 

\begin{equation}
\tag{4}
    BIC({H}_{i}) = -2log{L}_{i} + {k}_{i}log(n)
\end{equation}

where $n$ is the number of observations, ${df}_{i}$ is the number of free parameters of model $M_{i}$, ${L}_{i}$ is the maximum likelihood for model $M_{i}$.

#### Degrees of Freedom 
Calculating model degrees of freedom, ${df}_{i}$, is not straightforward for `gam` with a smoothing spline. In practice k-1 sets the upper limit on the degrees of freedom associated with an `s()` smooth (1 degree of freedom is usually lost to the identifiability constraint on the smooth). However the actual effective degrees of freedom are controlled by the degree of penalization selected during fitting, by GCV. In the case of the default smooth term with k=4, `s(x,k=4)`, 3 basis functions are used to estimate the effect of $x$ on the outcome variable. The upper limit for effective degrees of freedom (edf) can be at most 3 based on the penalty and edf=1 represents a linear relationship between the variables. 

`logLik` function in R is used to extract ${L}_{i}$ for each model and it is applicable to both `lm` and `gam` objects. If a `gam` object includes smooth term, `logLik` corrects the degrees of freedom. When testing a linear relationship with an `s()` smooth, `gam` is supposed to result in a linear fit with the same ${L}_{i}$ and ${df}_{i}$ as in `gam` without the smooth term. In other words, when the true relationship between $x$ and $y$ is linear, BIC for the models $Y \sim x$ and $Y \sim s(x)$ will be the same meaning that Bayes factor between them will be 1. However, `gam` with the smooth term is still a more complex model because compared to a linear model fit we are estimating 2 more free parameters in the case of k=4. For the variable selection problem considered in this work,the model with the smoothing spline is needed to be penalized to choose  the simpler model. Thus, edf is fixed to its maximum value 3 for each smooth which is supposed to be 1 for a true linear relationship. 

\
\

The remaining part is consisted of the simulation studies to check whether $B_{ij}$ is consistent for identifying the effect types between the variables in different settings. The results showed that the proposed Bayes factor is able identify the type of the effect of the predictor variable on the outcome variable consistently across different scenarios. 

### Libraries

The required packages have to be loaded. The main interest of the present work is the `gam` function from the R package `mgcv`. 

```{r, message = FALSE}
library(mgcv)
```

## Null model

First, a null model is tested against the linear and nonlinear models to check whether the Bayes factor can identify the zero effect. The following two models are tested against each other;

\

\begin{equation}
\tag{5}
M_{0}:lm(y \sim 1) \\
M_{1}:lm(y \sim 1 + x_{1} ) \\
M_{2}:gam(y \sim 1 + s(x_{1}))
\end{equation}

\

where $y \sim N(1, 0.1)$ and  $x_{1} \sim N(1, 0.5)$ implying $M_{0}$ is the true model. 

\

$M_{1}$ and $M_{2}$ are tested against $M_{0}$ (the true model) using BIC approx of Bayes factor. We expect $BF_{01}$ and $BF_{02}$ result in favor of model $M_{0}$. 

```{r null model testing}
null_model_test <- function(n = 100, sigma2 = 0.1){ #default n=100 and sigma2=0.1
   
   output_mat_null <- matrix(NA, nrow = 500, ncol = 2)
   colnames(output_mat_null) <- c("B01", "BF02")
   
   for (j in 1:500){ #for 500 different datasets 
     data <- as.data.frame(cbind("y" = rnorm(n, 1, sigma2),#data generation 
                                 "x1" = rnorm(n, 0, 0.5)))
     
     #model fit
     null_model <- lm(y ~ 1, data = data)
     linear_model <- lm(y ~  1 + .,data = data)
     non_linear_model <- gam(y~ 1 + s(x1, k=4), data = data)
     
     #BIC calculation 
     bic_null <- (-2) * head(logLik(null_model)) +  attr(logLik(null_model), "df")* log(n) #H0 
     bic_lin <- (-2) * head(logLik(linear_model)) +  attr(logLik(linear_model), "df")* log(n) #H1
     bic_nonl <- (-2) * head(logLik(non_linear_model)) +  (attr(logLik(linear_model), "df") + 2) * log(n) #H2
     
     
     B01 <- log(exp((bic_lin - bic_null) /2)) #null against the linear
     BF02 <- log(exp((bic_nonl - bic_null) /2)) #null against the nonlinear model 
     
     output_mat_null[j, 1] <- B01
     output_mat_null[j, 2] <- BF02
     }
   
   return(output_mat_null)
 }  
```


#### Consistency along diferrent sample sizes, $\{n \in 50, 100, 500, 1000\}$

We expect $log(BF_{01})$ and $log(BF_{02})$ are greater than zero and increasing with higher sample sizes with $\sigma^2 = 0.1$

```{r sample size identification}
 n_sizes <- c(50, 100, 500, 1000) #sample sizes 
```

```{r sample size evaluations, fig.dim=c(6, 4), eval=FALSE}
 null_samp <- matrix(NA, nrow = 500, ncol = 8)  #matrix for the results 
 colnames(null_samp) <- c("B01_50","BF02_50", "B01_100", "BF02_100", 
                          "B01_500", "BF02_500", "B01_1000", "BF02_1000")
 for (i in 1:4){
   null_samp[,c((i + i-1), (2*i))] <- null_model_test(n=n_sizes[i])
 }

```

```{r sample size results}
colMeans(null_samp)
```

```{r sample size plots, fig.dim=c(6,4)}

#PLots 
plot(density(null_samp[,1]), 
     xlim = c(min(null_samp), max(null_samp)), ylim=c(0, 1.7),
     main = "log(BF01) for different sample sizes", col = "red",
     xlab = "log(BF01)")
lines(density(null_samp[,3]), col = "darkblue")
lines(density(null_samp[,5]), col = "magenta")
lines(density(null_samp[,7]))
legend("topright", col = c("red", "darkblue", "magenta", "black"), cex=.80,
       lty=c(1,2,1,1),lwd=c(2,2,1,1), inset=.01,
       legend = c("n = 50", "n = 100", "n = 500", "n = 1000"))


plot(density(null_samp[,2]),
     xlim = c(min(null_samp), max(null_samp)), ylim=c(0, 1.7),
     main = "log(BF02) for different sample sizes", col = "red",
     xlab = "log(BF02)")
lines(density(null_samp[,4]), col = "darkblue")
lines(density(null_samp[,6]), col = "magenta")
lines(density(null_samp[,8]))
legend("topright", col = c("red", "darkblue", "magenta", "black"), cex=.80,
       lty=c(1,2,1,1),lwd=c(2,2,1,1), inset=.01,
       legend = c("n = 50", "n = 100", "n = 500", "n = 1000"))
```

* As $n$ increases, both $BF_{01}$ and $BF_{02}$ increase meaning that the evidence for the null model increases. On average, evidence against the nonlinear model is higher which is expected as the nonlinear model has higher degrees of freedom with relatively the same loglikelihood value.

#### Consistency along different $\sigma^2$ values, $\{\sigma^2 \in seq(.01, 1, 10)\}$ 

```{r sigma2 identification}
sigma2 <- c(seq(from = 0.01,  to = 1, length.out = 10)) #sigma2 sequence
```



```{r sigma2 evaluation,  eval=FALSE}
null_sigma <- matrix(NA, nrow = 500, ncol = 20) #matrix for the results 
colnames(null_sigma) <- c("B01, s2=0.01", "BF02, s2=0.01",
                          "B01, s2=0.12", "BF02, s2=0.12",
                          "B01, s2=0.23", "BF02, s2=0.23",
                          "B01, s2=0.34", "BF02, s2=0.34",
                          "B01, s2=0.45", "BF02, s2=0.45",
                          "B01, s2=0.56", "BF02, s2=0.56",
                          "B01, s2=0.67", "BF02, s2=0.67",
                          "B01, s2=0.78", "BF02, s2=0.78",
                          "B01, s2=0.89", "BF02, s2=0.89",
                          "B01, s2=1.00", "BF02, s2=1.00")
for (i in 1:10){
  null_sigma[, c((i + i-1), (2*i))] <- null_model_test(sigma2 = sigma2[i])
}

```

```{r sigma2 plots, fig.dim=c(6, 4)}

plot(x = sigma2, y=colMeans(null_sigma[,c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)]),
     type = "l", col="red", xlab = "sigma2", ylab = "log(B01)",  
     main = "log(BF01) for different sigma2")

plot(x = sigma2, y=colMeans(null_sigma[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)]),
     type = "l", col="blue", xlab = "sigma2", ylab = "log(BF02)",
     main = "log(BF02) for different sigma2")

```


* $log(BF_{01})$ and $log(BF_{02})$ are greater than zero. However, $\sigma^2$ does not have a systematic effect on the relative evidence for the null model against both linear and nonlinear models. This is actually expected as there is a zero relationship between the predictor variables and $y$. So the variance change occurring in the outcome variables still does not change the results. Also, notice that $log(BF_{02})$ remains higher than $log(BF_{01})$.  

## Linear non-zero Model 

A linear model is tested against a nonlinear model where the true relationship is a non-zero linear effect. The following two models are tested against each other;  

\begin{equation}
\tag{6}
M_0: lm(y \sim 1+x1) \\
M_1: gam(y \sim 1+ s(x1))
\end{equation}

where $x_{1} \sim N(1, 0.5)$,  $y = \beta_{1} x1 + \epsilon$ and $\epsilon \sim N(0, 0.1)$. So $M_0$ is the true model and $BF_{01}$ is expected to result in favor of $M_0$. 

```{r linear model function, eval=TRUE}
linear_model_test <- function(beta = 1.5, n = 100, sigma2 = 0.1){
  BF01s <- c(rep(NA, 500))
  for (j in 1:500){
    error <- rnorm(n,sd=sqrt(sigma2))
    x1 = rnorm(n, 0, 0.5)
    y <- beta*x1 + error
    data <- as.data.frame(cbind(y, x1))
    
    #model fit
    linear_model <- lm(y ~ 1 + x1, data = data) #H0 
    non_linear_model <- gam(y~ 1 + s(x1, k=4), data = data) #H1
    
    #BIC calculation 
    bic_lin <- (-2) * head(logLik(linear_model)) + 3* log(n) #H0
    bic_nonl <- (-2) * head(logLik(non_linear_model)) +  5 * log(n) #H1
    
    logLik(linear_model);logLik(non_linear_model)
    
    BF01 <- log(exp((bic_nonl - bic_lin) /2)) #BF_{linear against nonlinear} higher values indicates more evidence for the linear model
    BF01s[j] <- BF01}
  
  return(BF01s)
}
```

#### Consistency along diferrent sample sizes, $\{n \in 50, 100, 500, 1000\}$

```{r, eval=FALSE}
linear_samp <- matrix(NA, nrow = 500, ncol = 4) #matrix for the results  
colnames(linear_samp) <- c("BF01_50","BF01_100", "BF01_500", "BF01_1000")

for (i in 1:4){
  linear_samp[,i] <- linear_model_test(n = n_sizes[i])
}

write.csv(linear_samp, "linear_samp.csv")
```


```{r}
colMeans(linear_samp)
```

* As $n$ increases, $BF_{01}$ also increases meaning that the evidence for the linear model is higher for higher sample sizes. 


#### Consistency along different $\beta$ values, $\{\beta \in seq(-3, 3, 10)\}$

```{r}
betas <- seq(-3, 3, length.out = 10) #beta sequence
```

```{r, eval=FALSE}
linear_beta <- matrix(NA, nrow=500, ncol=10) #matrix for the results 
for (i in 1:10){
  linear_beta[,i] <- linear_model_test(beta = betas[i])
}
write.csv(linear_beta, "linear_beta.csv")
```

```{r, fig.dim=c(6, 4)}
plot(y=colMeans(linear_beta), x=betas, type="l", ylab="log(BF01)", xlab="beta", main="log(BF01) across Beta values")
```



#### Consistency along different $\sigma^2$ values, $\{\sigma^2 \in seq(.01, 1, 10)\}$ 

```{r}
sigma2 <- c(seq(from = 0.01,  to = 1, length.out = 10)) #sigma2
```

```{r, eval=FALSE}
linear_sigma <- matrix(NA, nrow=500, ncol=10)
colnames(linear_sigma) <- as.character(sigma2)
for (i in 1:10){
  linear_sigma[,i] <- linear_model_test(sigma2 = sigma2[i])
}

write.csv(linear_sigma, "linear_sigma.csv")
```

```{r, fig.dim=c(6, 4)}
plot(y=colMeans(linear_sigma), x=sigma2, type = "l", xlab="sigma2", ylab="log(BF01)", main="log(BF01) across sigma2 values")

```

* Both $\sigma^2$ and $\beta$ does not have an effect on Bayes factor as the relationship between `x1` and `y` is linear. When the relationship is linear both `gam` and `lm` will produce almost the same loglikelihood estimates. $\sigma^2$ and $\beta$ will effect both `gam` and `lm` loglikelihood in the same direction and with the same amount. So the relative ratio of loglikelihoods will not change with different $\sigma^2$ and $\beta$ values. 


## Non-linear non-zero model

To check whether the Bayes factor is able to identify the nonlinear relationship, five different non-linear models are specified by using logarithmic, exponential, quadratic, sine, and a nonlinear model. All of them are tested against the linear model. The linear model is denoted as $M_0$ and the non-linear models for each relationship are denoted as $M_1$. So, $log(BF_{01})$ is expected to be close to zero. 

```{r nonlinear model function, eval=TRUE}
non_linear_test <- function(beta = 1.5, sigma2 = 0.1, n= 100){
  #default beta= 1, n=100 and sigma2=0.5
  output_mat_nonl <- matrix(NA, nrow = 500, ncol = 6)
  colnames(output_mat_nonl) <- c("BF01_lin", "BF01_log", "BF01_exp", "BF01_quad", "BF01_sine", "BF01_norm")
  for (j in 1:500){
    
    x1 = rnorm(n, 0, 0.5)
    error <- rnorm(n,sd=sqrt(sigma2)) #error terms
    y_lin <- beta*x1 + error  #linear 
    y_log <- beta*log(x1) + error #logarithmic
    y_exp <- beta*exp(x1) + error #exponential
    y_quad <- beta*x1^4 + error #quadratic
    y_sine <- beta*sin(x1/3) + error #sine
    y_norm <- beta*dnorm(x1)*x1 + error #a nonlinear model
    data <- as.data.frame(cbind(y_lin, y_log, y_exp, y_quad, y_sine, y_norm, x1))
    
    #################################################################################
    
    #Model fits
    
    #linear
    linear_model_lin <- lm(y_lin ~ 1 + x1, data=data)
    non_linear_model_lin <- gam(y_lin ~ 1 + s(x1, k= 4), data=data)
    non_linear_model_lin$edf
    #logarithmic
    linear_model_log <- lm(y_log ~ 1 + x1, data = data)
    non_linear_model_log <- gam(y_log ~ 1 + s(x1, k=-1), data=data)
    
    #exponential
    linear_model_exp <- lm(y_exp ~ 1 + x1, data = data)
    non_linear_model_exp <- gam(y_exp ~ 1 + s(x1, k=4), data=data)
    
    #quadratic
    linear_model_quad <- lm(y_quad ~ 1 + x1, data = data)
    non_linear_model_quad <- gam(y_quad ~ 1 +  s(x1, k=4), data=data)
    
    #sine
    linear_model_sine <- lm(y_log ~ 1 + x1, data = data)
    non_linear_model_sine <- gam(y_log~ 1 + s(x1, k=4), data=data)
    
    #a-nonlinear
    linear_model_norm <- lm(y_log ~ 1 + x1, data = data)
    non_linear_model_norm <- gam(y_log~ 1 + s(x1, k=4), data=data)
    
    #################################################################################
    
    #BIC calculations
    
    #linear
    bic_lin <- (-2) * head(logLik(linear_model_lin)) + 3*log(n) #H0
    bic_nonl <- (-2) * head(logLik(non_linear_model_lin)) + 5*log(n) #H1
    BF01_lin <- log(exp((bic_nonl - bic_lin) /2))
    output_mat_nonl[j,1] <- BF01_lin
    
    #logarithmic
    bic_lin_log <- (-2) * head(logLik(linear_model_log)) +  3*log(n) #H0
    bic_nonl_log <- (-2) * head(logLik(non_linear_model_log)) + 5*log(n) #H1
    BF01_log <- log(exp((bic_nonl_log - bic_lin_log) /2))
    output_mat_nonl[j,2] <- BF01_log
    
    #exponential 
    bic_lin_exp <- (-2) * head(logLik(linear_model_exp)) +  3*log(n) #H0
    bic_nonl_exp <- (-2) * head(logLik(non_linear_model_exp)) +  5*log(n) #H1
    BF01_exp <- log(exp((bic_nonl_exp - bic_lin_exp) /2))
    output_mat_nonl[j,3] <- BF01_exp
    
    #quadratic
    bic_lin_quad <- (-2) * head(logLik(linear_model_quad)) +  3*log(n) #H0
    bic_nonl_quad <- (-2) * head(logLik(non_linear_model_quad)) + 5*log(n) #H1
    BF01_quad <- log(exp((bic_nonl_quad - bic_lin_quad) /2))
    output_mat_nonl[j,4] <- BF01_quad
    
    #sine
    bic_lin_sine <- (-2) * head(logLik(linear_model_sine)) + 3*log(n) #H0
    bic_nonl_sine <- (-2) * head(logLik(non_linear_model_sine)) + 5*log(n) #H1
    BF01_sine <- log(exp((bic_nonl_sine - bic_lin_sine) /2))
    output_mat_nonl[j,5] <- BF01_sine
    
    #a-nonlinear
    bic_lin_norm <- (-2) * head(logLik(linear_model_norm)) +3*log(n) #H0
    bic_nonl_norm <- (-2) * head(logLik(non_linear_model_norm)) + 5*log(n) #H1
    BF01_norm <- log(exp((bic_nonl_norm - bic_lin_norm) /2))
    output_mat_nonl[j,6] <- BF01_sine

  }
  return(output_mat_nonl)
}
```

#### Consistency along diferrent sample sizes, $\{n \in 50, 100, 500, 1000\}$

```{r, message=FALSE, warning=FALSE, eval=FALSE}
B0_samp <- c() #vector for the results 
for (i in 1: 4){
  samp <- non_linear_test(n = n_sizes[i])
  B0_samp <- cbind(B0_samp, samp)
}
```

```{r, fig.dim=c(6, 4), eval=TRUE}
n_means<- colMeans(B0_samp)
plot(c(seq(from =50, to = 1000, length = 4)), n_means[c(1, 7, 13, 19)], ylim = c(min(n_means),max(n_means)),
     type = "l", col = "red", main = "log(BF01) for different sample sizes", 
     ylab = "log(BF01)", xlab = "Sample Size (n)") #linear
lines(c(seq(from =50, to = 1000, length = 4)), n_means[c(2, 8, 14, 20)], col = "darkblue") #logarithmic
lines(seq(from =50, to = 1000, length = 4), n_means[c(3, 9, 15, 21)], col = "magenta") #exponential
lines(seq(from =50, to = 1000, length = 4), n_means[c(4, 10, 16, 22)], col= "orange") #quadratic
lines(seq(from =50, to = 1000, length = 4), n_means[c(5, 11, 17, 23)], col = "pink") #sine
lines(seq(from =50, to = 1000, length = 4), n_means[c(6, 12, 18, 24)]) #a-nonlinear 
lines(seq(from =50, to = 1000, length = 4), y = c(rep(2.3, 4)),  col = "green", lwd = 2, lty = 2) #BF01 threshold fixed at 2.3.
legend(100, -200, legend=c("linear", "logarithmic", "exponential", "quadratic", "sine", "a-nonlinear", "BF01threshold=2.3"), 
       col = c("red", "darkblue", "magenta", "orange", "pink", "black", "green"), lty=1:2, cex=0.7)

```

* As $n$ increases, $BF_{01}$ decreases for nonlinear linear models meaning that the evidence for the linear model is lower for higher sample sizes. 

```{r, message=FALSE, warning=FALSE, eval=FALSE}
betas <- seq(-3, 3, length.out = 10) #beta sequence
nonlinear_beta <-c() #vector for the results
for (i in 1:10){
  nonlinear_beta <- cbind(nonlinear_beta, non_linear_test(beta = betas[i]))
}
write.csv(nonlinear_beta, "nonlinear_beta.csv")
```

#### Consistency along different $\beta$ values, $\{\beta \in seq(-3, 3, 10)\}$

```{r, eval=TRUE, fig.dim=c(6, 4)}
beta_means <- colMeans(nonlinear_beta)
plot(seq(from =-3, to = 3, length = 10), beta_means[c(1, 7, 13, 19, 25, 31, 37, 43, 49, 55)], ylim = c(min(beta_means),max(beta_means)),
     type = "l", col = "red", main = "log(BF01) for different Beta values", 
     ylab = "log(BF01)", xlab = "Beta") #linear
lines(seq(from =-3, to = 3, length = 10), 
      beta_means[c(2, 8, 14, 20, 26, 32, 38, 44, 50, 56)], col = "darkblue") #logarithmic
lines(seq(from =-3, to = 3, length = 10), 
      beta_means[c(3, 9, 15, 21, 27, 33, 39, 45, 51, 57)], col = "magenta") #exponential
lines(seq(from =-3, to = 3, length = 10), 
      beta_means[c(4, 10, 16, 22, 28, 34, 40, 46, 52, 58)], col= "orange") #quadratic
lines(seq(from =-3, to = 3, length = 10), 
      beta_means[c(5, 11, 17, 23, 29, 35, 41, 47, 53, 59)], col = "pink") #sine
lines(seq(from =-3, to = 3, length = 10), 
      beta_means[c(6, 12, 18, 24, 30, 36, 42, 48, 54, 60)]) #a-nonlinear 
lines(seq(from =-3, to = 3, length = 10), y = c(rep(2.3, 10)),  col = "green", lwd = 2, lty = 2) #BF01 threshold fixed at 2.3.
legend(-0.75, -30, legend=c("linear", "logarithmic", "exponential", "quadratic", "sine", "a-nonlinear", "BF01threshold=2.3"), 
       col = c("red", "darkblue", "magenta", "orange", "pink", "black", "green"), lty=1:2, cex=0.7)

```

* As $\beta$ moves away from the zero point, $BF_{01}$ decreases for nonlinear models meaning that the evidence in favor nonlinear effect increases. $BF_{01}$ for the linear model, again, remains constant as in the linear non-zero model case above. Notice that $BF_{01}$ is still in favor of linear model when $\beta = 0$. This is caused by the penalization applied to the nonlinear model by counting `df`=$3$ for each spline. 

#### Consistency along different $\sigma^2$ values, $\{\sigma^2 \in seq(.01, 1, 10)\}$

```{r, message=FALSE, warning = FALSE, eval=FALSE}
sigma2 <- c(seq(from = 0.01,  to = 1, length.out = 10)) #sigma2
B0_sigma <- c() #vector for the results
for(i in 1: 10){
  sigmax <- non_linear_test(sigma2 = sigma2[i])
  B0_sigma <- cbind(B0_sigma, sigmax)
}
write.csv(B0_sigma, "B0_sigma.csv")
```

```{r, fig.dim=c(6, 4), eval=TRUE}
sigma2_means <- colMeans(B0_sigma)
plot(sigma2, y=sigma2_means[c(1, 7, 13, 19, 25, 31, 37, 43, 49, 55)], ylim = c(min(sigma2_means), max(sigma2_means)), 
     type = "l", col = "red",main ="log(BF01) for different sigma values", 
     ylab = "log(BF01)", xlab = "sigma2") #linear
lines(sigma2, y = sigma2_means[c(2, 8, 14, 20, 26, 32, 38, 44, 50, 56)], col = "darkblue") #logarithmic
lines(sigma2, y = sigma2_means[c(3, 9, 15, 21, 27, 33, 39, 45, 51, 57)], col = "magenta") #exponential
lines(sigma2, y = sigma2_means[c(4, 10, 16, 22, 28, 34, 40, 46, 52, 58)], col = "orange") #quadratic 
lines(sigma2, y= sigma2_means[c(5, 11, 17, 23, 29, 35, 41, 47, 53, 59)], col = "pink") #sine 
lines(sigma2, y = sigma2_means[c(6, 12, 18, 24, 30, 36, 42, 48, 54, 60)]) #a-nonlinear
lines(sigma2, y = c(rep(2.3, 10)), col = "green", lwd = 2, lty = 2) #BF01 threshold fixed at 2.3. 
legend(0.5, -40,
       legend=c("linear", "logarithmic", "exponential", "quadratic", "sine", "a-nonlinear", "BF01threshold=2.3"), 
       col = c("red", "darkblue", "magenta", "orange", "pink", "black", "green"), lty=1:2, cex=0.7)

```

* As the variance in the outcome variable increases, $BF_{01}$ increases for nonlinear models but it still remains in favor of nonlinear models. $BF_{01}$ for the linear model remains constant as in the linear non-zero model case above. 

## Model with three variables: A-non-zero-linear; A-non-zero-(non)linear; A-zero effect

Now we will try to identify the type of the effect of each variable simultaneously using 3-variable model where the true model is identified as: 
\begin{equation}
\tag{7}
y = \beta_0+\beta_1exp(x_1)+\beta_2x_2+\beta_3x_3+ \epsilon
\end{equation}

where $x_{1} \sim N(1, 0.5)$;  $x_{2} \sim N(1, 0.5)$; $x_{3} \sim N(1, 0.5)$; $\beta_i \in {2,2,0}$ and $\epsilon \sim N(0, 0.1)$. In words, there is a non-linear non-zero effect of $x_1$, a linear non-zero effect of $x_2$ and zero effect of $x_3$ on the outcome variable. 

There are three predictor variables that can have three possible different effect on the outcome variables. Thus, $3^{3}$ possible models to be specified. The true model is expected to have the highest Bayes factor compared other 26 models. 


```{r complex text function}
#Matrix for average Bayes Factors for each model against all other models 
mean_BFs <- matrix(NA, nrow = 500, ncol = 27)
colnames(mean_BFs) <- c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", 
                        "M10","M11", "M12", "M13","M14", "M15", "M16", "M17", "M18",
                        "M19", "M20", "M21", "M22", "M23", "M24", "M25", "M26", "M27")
complex_test <- function(n = 100, sigma2 = 0.1, k=4, b=2){#default n=100 and sigma2=0.1
  for (j in 1:500){
    
    features <- cbind("x1" = rnorm(n, 0, .5), "x2" = rnorm(n, 0, .5), "x3" = rnorm(n, 0, .5))
    beta <- c(b, b, 0) #discuss how to adjust beta values 
    error <- rnorm(n,sd=sqrt(sigma2))
    y <- beta[1]*exp(features[,1]) + beta[2]*features[,2] + beta[3]*features[,3] + error
    data <- as.data.frame(cbind(y, features))
    
    #model fits & BIC calculations
    bics <- c(rep(NA, 27)) #vector for bic scores of the models fitted below
     
    M1 <- gam(y ~ 1,data=data); bics[1]=(-2)*head(logLik(M1))+2*log(n)
    
    M2 <- gam(y ~ 1 + x1, data=data); bics[2]=(-2)*head(logLik(M2))+3*log(n)
    M3 <- gam(y ~ 1 + x2, data=data); bics[3]=(-2)*head(logLik(M3))+3*log(n)
    M4 <- gam(y ~ 1 + x3, data=data); bics[4]=(-2)*head(logLik(M4))+3*log(n)
    
    M5 <- gam(y ~ 1 + s(x1, k=k), data=data);bics[5]=(-2)*head(logLik(M5))+5*log(n)
    M6 <- gam(y ~ 1 + s(x2, k=k), data=data);bics[6]=(-2)*head(logLik(M6))+5*log(n) 
    M7 <- gam(y ~ 1 + s(x3, k=k), data=data);bics[7]=(-2)*head(logLik(M7))+5*log(n)
    
    M8 <- gam(y ~ 1 + x1 + x2, data=data); bics[8]=(-2)*head(logLik(M8))+4*log(n)
    M9 <- gam (y ~ 1 + x1 + x3, data=data); bics[9]=(-2)*head(logLik(M9))+4*log(n)
    M10 <- gam(y ~ 1 + x2 + x3, data=data); bics[10]=(-2)*head(logLik(M10))+4*log(n)

    M11 <- gam(y ~ 1 + s(x1, k=k) + x2, data=data); bics[11]=(-2)*head(logLik(M11))+6*log(n) #true
    M12 <- gam(y ~ 1 + s(x1, k=k) + x3, data=data); bics[12]=(-2)*head(logLik(M12))+6*log(n)

    M13 <- gam(y ~ 1 + x1 + s(x2, k=k), data=data); bics[13]=(-2)*head(logLik(M13))+6*log(n)
    M14 <- gam(y ~ 1 + x1 + s(x3, k=k), data=data); bics[14]=(-2)*head(logLik(M14))+6*log(n)
    
    M15 <- gam(y ~ 1 + s(x2, k=k) + x3, data=data); bics[15]=(-2)*head(logLik(M15))+6*log(n)
    M16 <- gam(y ~ 1 + x2 + s(x3, k=k), data=data); bics[16]=(-2)*head(logLik(M16))+6*log(n)
    
    M17 <- gam(y ~ 1 + s(x1, k=k) + s(x2, k=k), data=data); bics[17]=(-2)*head(logLik(M17))+8*log(n)
    M18 <- gam(y ~ 1 + s(x1, k=k) + s(x3, k=k), data=data); bics[18]=(-2)*head(logLik(M18))+8*log(n)
    M19 <- gam(y ~ 1 + s(x2, k=k) + s(x3, k=k), data=data); bics[19]=(-2)*head(logLik(M19))+8*log(n)
    
    M20 <- gam(y ~ 1 + x1 + x2 + x3, data = data); bics[20]=(-2)*head(logLik(M20))+5*log(n)
    
    M21 <- gam(y ~ 1 + s(x1, k=k) + x2 + x3, data=data); bics[21]=(-2)*head(logLik(M21))+7*log(n)
    M22 <- gam(y ~ 1 + x1 + s(x2, k=k) + x3, data=data); bics[22]=(-2)*head(logLik(M22))+7*log(n)
    M23 <- gam(y ~ 1 + x1 + x2 + s(x3, k=k), data=data); bics[23]=(-2)*head(logLik(M23))+7*log(n)
    
    M24 <- gam(y ~ 1 + s(x1, k=k) + s(x2, k=k) + x3, data=data); bics[24]=(-2)*head(logLik(M24))+9*log(n)
    M25 <- gam(y ~ 1 + s(x1, k=k) + x2 + s(x3, k=k), data=data); bics[25]=(-2)*head(logLik(M25))+9*log(n)
    M26 <- gam(y ~ 1 + x1 + s(x2, k=k) + s(x3, k=k), data=data); bics[26]=(-2)*head(logLik(M26))+9*log(n)
    
    M27 <- gam(y ~ 1 + s(x1, k=k) + s(x2, k=k) + s(x3, k=k), data=data); bics[27]=(-2)*head(logLik(M27))+11*log(n)
    
    #matrix for Bayes Factors 
    BFs <- matrix(NA, nrow = 27, ncol = 27)
    colnames(BFs) <- c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", 
                       "M9", "M10","M11", "M12", "M13", "M14", "M15", "M16", "M17", "M18",
                       "M19", "M20", "M21", "M22", "M23", "M24", "M25", "M26", "M27")
    rownames(BFs) <- c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", 
                       "M9", "M10","M11", "M12", "M13", "M14", "M15", "M16", "M17", "M18",
                       "M19", "M20", "M21", "M22", "M23", "M24", "M25", "M26", "M27")
    
    
    for (k in 1:27) {
      bf_vec<-c()
      for (x in 1:26) {
        bf <- log(exp((bics[-k][x] - bics[k]) /2)) 
        bf_vec <- c(bf_vec, bf)
      }
      bf_vec <- R.utils::insert(bf_vec, ats=k, values=0) #inserting 0 to the diagonals
      BFs[, k] <- round(bf_vec, digits=3)
    }
    
  BFs[BFs == "-Inf"]  <- -1e5; BFs[BFs == "Inf"]  <- 1e5 #to deal with infinite values
  mean_BFs[j,] <- colMeans(BFs)
    
    result <- list("the best model chosen" = sort(colMeans(mean_BFs))[27],
                   "Mean Bayes factors for the models (unsorted" = colMeans(mean_BFs),
                   "Mean Bayes factors for the models (sorted)" = sort(colMeans(mean_BFs)),
                   "Bayes Factors for each trial" = mean_BFs)
  }
  return(result)
}
```



#### Consistency along diferrent sample sizes, $\{n \in 50, 100, 500, 1000\}$

```{r sample size evaluation1, eval=FALSE}
n_vec_BF <- c(rep(NA, 4)) #for the BF value of the best model chosen
n_vec_name <- c(rep(NA, 4)) #exact name of the best model chosen
n_sizes <- c(100, 250, 500, 1000)
for (i in 1:4){
  x <- complex_test(n = n_sizes[i])
  n_vec_BF[i] <- x[[1]] 
  n_vec_name[i] <- names(x[[1]])
}
names(n_vec_BF) <- n_vec_name
```
```{r sample size results1}
n_vec_BF
```

* As $n$ increases Bayes factor for the true model, `M11`, gets higher. 

#### Consistency along different $\beta$ values, $\{\beta \in seq(-3, 3, 10)\}$

```{r beta evaluation1, eval=FALSE}
betas <- seq(-3, 3, length.out = 10) #beta sequence
beta_vec_BF <- c(rep(NA, 10)) #for the BF value of the best model chosen
beta_vec_name <- c(rep(NA, 10)) #exact name of the best model chosen

for (i in 1:10){
  x<-complex_test(b = betas[i])
  beta_vec_BF[i] <- x[[1]]
  beta_vec_name[i] <- names(x[[1]])
}

names(beta_vec_BF) <- beta_vec_name
```
```{r beta results1}
beta_vec_BF
```

* As $\beta$ increases, the Bayes factor for the true model M11 also increases. However, it is worth noting that when $\beta$ is very low, the model M8 obtains the highest posterior probability. This implies that the algorithm struggles to capture the nonlinearity in the data. While it can recognize the relevance of the variable $x_1$, it fails to identify its nonlinear effect on $y$.

\
\
\


### References 
 
* Wagenmakers, E.-J. 2007. A practical solution to the pervasive problems of p values. Psychonomic Bulletin & Review, 14, 779–804.
* Wood, S. N. (2003). Thin Plate Regression Splines. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 65(1), 95–114. http://www.jstor.org/stable/3088828


\
\
\

